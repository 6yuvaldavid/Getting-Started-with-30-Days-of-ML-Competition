{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Welcome to the **[30 Days of ML competition](https://www.kaggle.com/c/30-days-of-ml/overview)**!  In this notebook, you'll learn how to make your first submission.\n\nBefore getting started, make your own editable copy of this notebook by clicking on the **Copy and Edit** button.\n\n# Step 1: Import helpful libraries\n\nWe begin by importing the libraries we'll need.  Some of them will be familiar from the **[Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning)** course and the **[Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning)** course.","metadata":{}},{"cell_type":"code","source":"# Familiar imports\nimport numpy as np\nimport pandas as pd\n\n# For ordinal encoding categorical variables, splitting data\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\n\n# For training random forest model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C","metadata":{"execution":{"iopub.status.busy":"2021-08-21T23:49:32.306077Z","iopub.execute_input":"2021-08-21T23:49:32.306486Z","iopub.status.idle":"2021-08-21T23:49:32.314132Z","shell.execute_reply.started":"2021-08-21T23:49:32.306449Z","shell.execute_reply":"2021-08-21T23:49:32.313052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Load the data\n\nNext, we'll load the training and test data.  \n\nWe set `index_col=0` in the code cell below to use the `id` column to index the DataFrame.  (*If you're not sure how this works, try temporarily removing `index_col=0` and see how it changes the result.*)","metadata":{}},{"cell_type":"code","source":"# Load the training data\ntrain = pd.read_csv(\"../input/30-days-of-ml/train.csv\", index_col=0)\ntest = pd.read_csv(\"../input/30-days-of-ml/test.csv\", index_col=0)\n\n# Preview the data\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T23:49:34.481533Z","iopub.execute_input":"2021-08-21T23:49:34.481888Z","iopub.status.idle":"2021-08-21T23:49:36.439806Z","shell.execute_reply.started":"2021-08-21T23:49:34.481856Z","shell.execute_reply":"2021-08-21T23:49:36.438674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next code cell separates the target (which we assign to `y`) from the training features (which we assign to `features`).","metadata":{}},{"cell_type":"code","source":"# Separate target from features\ny = train['target']\nfeatures = train.drop(['target'], axis=1)\n\n# Preview features\nfeatures.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T23:49:36.584064Z","iopub.execute_input":"2021-08-21T23:49:36.584406Z","iopub.status.idle":"2021-08-21T23:49:36.645177Z","shell.execute_reply.started":"2021-08-21T23:49:36.584375Z","shell.execute_reply":"2021-08-21T23:49:36.643888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: Prepare the data\n\nNext, we'll need to handle the categorical columns (`cat0`, `cat1`, ... `cat9`).  \n\nIn the **[Categorical Variables lesson](https://www.kaggle.com/alexisbcook/categorical-variables)** in the Intermediate Machine Learning course, you learned several different ways to encode categorical variables in a dataset.  In this notebook, we'll use ordinal encoding and save our encoded features as new variables `X` and `X_test`.","metadata":{}},{"cell_type":"code","source":"# List of categorical columns\nobject_cols = [col for col in features.columns if 'cat' in col]\n\n# ordinal-encode categorical columns\nX = features.copy()\nX_test = test.copy()\nordinal_encoder = OrdinalEncoder()\nX[object_cols] = ordinal_encoder.fit_transform(features[object_cols])\nX_test[object_cols] = ordinal_encoder.transform(test[object_cols])\n\n# Preview the ordinal-encoded features\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T23:41:37.116463Z","iopub.execute_input":"2021-08-21T23:41:37.116856Z","iopub.status.idle":"2021-08-21T23:41:41.221473Z","shell.execute_reply.started":"2021-08-21T23:41:37.11682Z","shell.execute_reply":"2021-08-21T23:41:41.220465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we break off a validation set from the training data.","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n#Scaling the features\n#from sklearn.preprocessing import StandardScaler\n#scaler = StandardScaler().fit(X_train)\n#X_train_scaled = pd.DataFrame(scaler.transform(X_train),\n#                            columns = X_train.columns)\n#X_train_scaled.head()\n#X_valid_scaled = pd.DataFrame(scaler.transform(X_valid),\n#                             columns = X_valid.columns)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T23:41:41.223298Z","iopub.execute_input":"2021-08-21T23:41:41.223746Z","iopub.status.idle":"2021-08-21T23:41:41.355395Z","shell.execute_reply.started":"2021-08-21T23:41:41.223702Z","shell.execute_reply":"2021-08-21T23:41:41.354403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Train a model\n\nNow that the data is prepared, the next step is to train a model.  \n\nIf you took the **[Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning)** courses, then you learned about **[Random Forests](https://www.kaggle.com/dansbecker/random-forests)**.  In the code cell below, we fit a random forest model to the data.","metadata":{}},{"cell_type":"code","source":"# Define the model \nmodel = XGBRegressor(n_estimators=2500, n_jobs=10, random_state=0)\n\n# Train the model (will take about 10 minutes to run)\nmodel.fit(X_train, y_train, early_stopping_rounds=10, eval_set=[(X_valid, y_valid)], verbose=0)\n\npreds_valid = model.predict(X_valid)\nprint(mean_squared_error(y_valid, preds_valid, squared=False))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:23:18.281198Z","iopub.execute_input":"2021-08-16T22:23:18.281968Z","iopub.status.idle":"2021-08-16T22:24:06.855842Z","shell.execute_reply.started":"2021-08-16T22:23:18.281908Z","shell.execute_reply":"2021-08-16T22:24:06.854452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We will save the model performance metrics in a DataFrame\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import KFold, cross_val_score\nimport numpy as np\nModel = []\nRMSE = []\nR_sq = []\ncv = KFold(5)\n\n#Creating a Function to append the cross validation scores of the algorithms\ndef input_scores(name, model, x, y):\n    Model.append(name)\n    RMSE.append(np.sqrt((-1) * cross_val_score(model, x, y, cv=cv, \n                                               scoring='neg_mean_squared_error').mean()))\n    R_sq.append(cross_val_score(model, x, y, cv=cv, scoring='r2').mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-17T00:41:24.629348Z","iopub.execute_input":"2021-08-17T00:41:24.62988Z","iopub.status.idle":"2021-08-17T00:41:24.637128Z","shell.execute_reply.started":"2021-08-17T00:41:24.629831Z","shell.execute_reply":"2021-08-17T00:41:24.635829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, \n                              AdaBoostRegressor)\nfrom sklearn.ensemble import VotingRegressor\n\n#names = ['Linear Regression', 'Ridge Regression', 'Lasso Regression',\n#         'K Neighbors Regressor', 'Decision Tree Regressor', \n#         'Random Forest Regressor', 'Gradient Boosting Regressor',\n#         'Adaboost Regressor']\n#models = [LinearRegression(), Ridge(), Lasso(),\n#          KNeighborsRegressor(), DecisionTreeRegressor(),\n#          RandomForestRegressor(), GradientBoostingRegressor(), \n#          AdaBoostRegressor()]\n#\n#Running all algorithms\n#for name, model in zip(names, models):\n#    input_scores(name, model, X_train_scaled, y_train)\n\nreg1 = GradientBoostingRegressor(random_state=1)\nreg2 = RandomForestRegressor(random_state=1)\nreg3 = LinearRegression()\nreg4 = Ridge()\nreg5 = Lasso()\nereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3), ('rid', reg4), ('las', reg5)])\nereg = ereg.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-17T07:37:49.340923Z","iopub.execute_input":"2021-08-17T07:37:49.341407Z","iopub.status.idle":"2021-08-17T07:52:09.551901Z","shell.execute_reply.started":"2021-08-17T07:37:49.341368Z","shell.execute_reply":"2021-08-17T07:52:09.550469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate a Gaussian Process model\nkernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\ngp = GaussianProcessRegressor()\n\n# Fit to data using Maximum Likelihood Estimation of the parameters\ngp.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T23:49:43.094969Z","iopub.execute_input":"2021-08-21T23:49:43.095314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_valid = gp.predict(X_valid)\nprint(mean_squared_error(y_valid, preds_valid, squared=False))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T23:42:41.315092Z","iopub.execute_input":"2021-08-21T23:42:41.315471Z","iopub.status.idle":"2021-08-21T23:42:42.89831Z","shell.execute_reply.started":"2021-08-21T23:42:41.315436Z","shell.execute_reply":"2021-08-21T23:42:42.897272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#evaluation = pd.DataFrame({'Model': Model,\n#                           'RMSE': RMSE,\n#                           'R Squared': R_sq})\n#print(\"FOLLOWING ARE THE TRAINING SCORES: \")\n#evaluation","metadata":{"execution":{"iopub.status.busy":"2021-08-17T03:50:00.072192Z","iopub.execute_input":"2021-08-17T03:50:00.072681Z","iopub.status.idle":"2021-08-17T03:50:00.091037Z","shell.execute_reply.started":"2021-08-17T03:50:00.072633Z","shell.execute_reply":"2021-08-17T03:50:00.089991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from lightgbm import LGBMRegressor\n\n# lgbm_parameters = {\n#     'metric': 'rmse', \n#     'n_jobs': -1,\n#     'n_estimators': 1160,\n#     'learning_rate': 0.01,\n#     'max_depth': 50,\n#     'num_leaves': 900,\n# }\n# early_sr=5\n\n\n# lgbm_model = LGBMRegressor(**lgbm_parameters)\n# lgbm_model.fit(X, y, eval_set = ((X_valid,y_valid)),verbose = -1, early_stopping_rounds = early_sr,categorical_feature=object_cols)\n\n# preds_valid = lgbm_model.predict(X_valid)\n# print(mean_squared_error(y_valid, preds_valid, squared=False))","metadata":{"execution":{"iopub.status.busy":"2021-08-17T06:19:13.947297Z","iopub.execute_input":"2021-08-17T06:19:13.947658Z","iopub.status.idle":"2021-08-17T06:21:24.628227Z","shell.execute_reply.started":"2021-08-17T06:19:13.947629Z","shell.execute_reply":"2021-08-17T06:21:24.627304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the code cell above, we set `squared=False` to get the root mean squared error (RMSE) on the validation data.\n\n# Step 5: Submit to the competition\n\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.","metadata":{}},{"cell_type":"code","source":"# Use the model to generate predictions\n#predictions = model.predict(X_test)\npredictions = lgbm_model.predict(X_test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T22:25:09.492739Z","iopub.execute_input":"2021-08-16T22:25:09.493096Z","iopub.status.idle":"2021-08-16T22:25:10.297422Z","shell.execute_reply.started":"2021-08-16T22:25:09.493064Z","shell.execute_reply":"2021-08-16T22:25:10.296183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once you have run the code cell above, follow the instructions below to submit to the competition:\n1. Begin by clicking on the **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  \n2. Ensure that the **Save and Run All** option is selected, and then click on the **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\nIf you want to keep working to improve your performance, select the **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.","metadata":{}},{"cell_type":"markdown","source":"# Step 6: Keep Learning!\n\nIf you're not sure what to do next, you can begin by trying out more model types!\n1. If you took the **[Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning)** course, then you learned about **[XGBoost](https://www.kaggle.com/alexisbcook/xgboost)**.  Try training a model with XGBoost, to improve over the performance you got here.\n\n2. Take the time to learn about **Light GBM (LGBM)**, which is similar to XGBoost, since they both use gradient boosting to iteratively add decision trees to an ensemble.  In case you're not sure how to get started, **[here's a notebook](https://www.kaggle.com/svyatoslavsokolov/tps-feb-2021-lgbm-simple-version)** that trains a model on a similar dataset.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}